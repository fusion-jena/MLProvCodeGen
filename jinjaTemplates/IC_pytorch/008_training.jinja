starttime = datetime.datetime.now()
seed_everything({{ seed }}, workers=True)
def set_train_log(log: str, value: str):
    log+= value
    return log

num_epochs = {{ num_epochs }}
epoch_log = ""
# Set up pytorch-ignite trainer and evaluator.
trainer = create_supervised_trainer(
    model,
    optimizer,
    loss_func,
    device=device,
    deterministic=True
)
{# TODO: Atm, the train metrics get accumulated, see torch_models.py #}
metrics = {
    "report": ClassificationReport(),
    "accuracy": Accuracy(),
    "loss": Loss(loss_func),
}
evaluator = create_supervised_evaluator(
    model, metrics=metrics, device=device
)

@trainer.on(Events.ITERATION_COMPLETED(every=print_every))
def log_batch(trainer):
    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1
    print(
        f"Epoch {trainer.state.epoch} / {num_epochs}, "
        f"batch {batch} / {trainer.state.epoch_length}: "
        f"loss: {trainer.state.output:.3f}"
    )

@trainer.on(Events.EPOCH_COMPLETED)
def log_epoch(trainer):
    print(f"Epoch {trainer.state.epoch} / {num_epochs} average results: ")
    train_log = set_train_log('', (f"Epoch {trainer.state.epoch} / {num_epochs} average results: "))

    def log_results(name, metrics, epoch):
        print(
            f"{name + ':':6} loss: {metrics['loss']:.3f}, "
            f"accuracy: {metrics['accuracy']:.3f}"
        )
        
        log = set_train_log(train_log, (
            f"{name + ':':6} loss: {metrics['loss']:.3f}, "
            f"accuracy: {metrics['accuracy']:.3f}"
        ))
        return log

    # Train data.
    evaluator.run(train_loader)
    log_results("train", evaluator.state.metrics, trainer.state.epoch)
    
    # Val data.
    if val_loader:
        evaluator.run(val_loader)
        log_results("val", evaluator.state.metrics, trainer.state.epoch)

    # Test data.
    if test_loader:
        evaluator.run(test_loader)
        log_results("test", evaluator.state.metrics, trainer.state.epoch)

    print()
    print("-" * 80)
    print()
    return train_log

{# TODO: Maybe use this instead: https://pytorch.org/ignite/handlers.html#ignite.handlers.ModelCheckpoint #}
{% if checkpoint %}
@trainer.on(Events.EPOCH_COMPLETED)
def checkpoint_model(trainer):
    torch.save(model, checkpoint_dir / f"model-epoch{trainer.state.epoch}.pt")

{% endif %}
# Start training.
train_log = trainer.run(train_loader, max_epochs=num_epochs)

print(train_log)
{% if visualization_tool == "Weights & Biases" %}
wandb.finish()
{% endif %}
endtime = datetime.datetime.now()
executionTime = endtime-starttime

for lines in str(train_log).splitlines():
    if 'seed:' in lines:
        resulting_model_seed = lines
        break
    else:
        resulting_model_seed = 'no_data'

e_training = d1.entity('ex:Cell Training', (
    ('ex:type', 'notebook_cell'),
    ('ex:type', 'p-plan:step'),
))
a_settraining = d1.activity('ex:set_training()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})
d1.wasStartedBy(a_settraining, e_training)
d1.hadMember(e_notebook, e_training)
e_training_data = d1.entity(
    'ex:Training Data',(
        ('ex:batch_size', batch_size),    
		('ex:epochs', num_epochs),
		#('ex:train_metrics', str(train_log)),
		('ex:print_progress', {{ print_every }}),
		('ex:seed', {{ seed }}),
		('ex:resulting_model_seed', resulting_model_seed),
))
d1.wasGeneratedBy(e_training_data, a_settraining)
d1.wasInfluencedBy(e_training, e_modelparameters_data)