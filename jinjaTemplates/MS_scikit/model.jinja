# Set up model, loss, optimizer.
model = models.{{ model_func }}(pretrained={{ pretrained }})
{# TODO: Maybe enable this by default, so that people can adapt num_classes afterward. #}
{% if num_classes != 1000 %}
num_classes = {{ num_classes }}
{% if "resnet" in model_func %}
model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)
{% elif "alexnet" in model_func or "vgg" in model_func %}
model.classifier[-1] = torch.nn.Linear(in_features=model.classifier[-1].in_features, out_features=num_classes, bias=True)
{% elif "densenet" in model_func %}
model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes, bias=True)
{% endif %}
{% endif %}
model = model.to(device)
loss_func = nn.{{ loss }}()
optimizer = optim.{{ optimizer }}(model.parameters(), lr=lr)

{% if visualization_tool == "Weights & Biases" %}
# Log gradients and model parameters to W&B.
wandb.watch(model)

{% endif %}


X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)
# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))

{# TODO: Atm, the train metrics get accumulated, see torch_models.py #}
metrics = {
    "accuracy": Accuracy(),
    "loss": Loss(loss_func),
}
evaluator = create_supervised_evaluator(
    model, metrics=metrics, device=device
)