{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18307b77",
   "metadata": {},
   "source": [
    "\n",
    "# Image Classification\n",
    "\n",
    "Building a machine learning model to solve Image Classification using the PyTorch framework.<br>\n",
    "Image Classification is one of the basic pattern recognition exercises. <br>\n",
    "Using Image files as its input, a model trained for Image classification will split a set of images into a given number of classes. <br>\n",
    "<br>\n",
    "This Notebook has been generated automatically using the JupyterLab extension ***MLProvCodeGen***.\n",
    "<br>\n",
    "The original Source Code is from this application https://github.com/jrieke/traingenerator <br>\n",
    "Made by: https://www.jrieke.com/ Twitter: https://twitter.com/jrieke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea8870",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Install required packages before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy===1.22.2 ipywidgets===7.6.5 torch===1.10.2 torchvision===0.11.3 pytorch-ignite===0.4.6 pytorch-lightning===1.5.10 gputil===1.4.0 psutil===5.9.0 py-cpuinfo===8.0.0 prov===2.0.0 pydot===1.4.2 --user\n",
    "#torch currently not supported with python 3.10, downgrading to python 3.9.7 possibly required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7bdc8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision as torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import ignite as pytorch_ignite\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, ClassificationReport\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import seed_everything\n",
    "import GPUtil\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import platform\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "import json\n",
    "import webbrowser\n",
    "import IPython\n",
    "from IPython.display import display, Image\n",
    "import prov\n",
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "import os\n",
    "from prov.model import ProvDocument, Namespace, Literal, PROV, Identifier\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520b621",
   "metadata": {},
   "source": [
    "### Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(bytes, suffix=\"B\"):\n",
    "    \"\"\"\n",
    "    Scale bytes to its proper format\n",
    "    e.g:\n",
    "        1253656 => '1.20MB'\n",
    "        1253656678 => '1.17GB'\n",
    "    \"\"\"\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "d1 = ProvDocument()\n",
    "d1.add_namespace('prov', 'http://www.w3.org/ns/prov#')\n",
    "d1.add_namespace('ex', 'https://github.com/TarekAlMustafa/MLProvCodeGen1.0/')\n",
    "d1.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')\n",
    "d1.add_namespace('p-plan', 'http://purl.org/net/p-plan')\n",
    "\n",
    "e_MLProvCodeGen = d1.entity(\n",
    "        'ex:MLProvCodeGen',(\n",
    "            ('prov:type', PROV['Plan']),\n",
    "))\n",
    "ag_author = d1.agent(\n",
    "        'ex:Tarek Al Mustafa',(\n",
    "            ('prov:type', PROV['Person']),\n",
    "            ('foaf:givenName', 'Tarek Al Mustafa'),\n",
    "            ('foaf:mbox', '<tarek.almustafa@uni-jena.de>'),\n",
    "            ('prov:role', 'Author'),\n",
    "))\n",
    "kernellist = !jupyter kernelspec list\n",
    "e_notebook = d1.entity(\n",
    "        'ex:notebook',(\n",
    "            ('ex:programming_language','Python'),\n",
    "            ('ex:programming_language_version', cpuinfo.get_cpu_info()['python_version']),\n",
    "            ('ex:kernel','python3(ipykernel)'),\n",
    "            ('prov:type', PROV['File']),\n",
    "            ('ex:fileformat', '.ipynb'),\n",
    "            ('ex:name', 'MulticlassClassification.ipynb'),\n",
    "            ('ex:creation_date', str(date.today())),\n",
    "            ('ex:last_modified', 'TODO'),\n",
    "))\n",
    "e_notebook.add_asserted_type('prov:Collection')\n",
    "d1.wasAttributedTo(e_notebook, ag_author)\n",
    "a_generateNotebook = d1.activity('ex:GenerateNotebook')\n",
    "d1.wasAssociatedWith(a_generateNotebook, ag_author, plan=e_MLProvCodeGen)\n",
    "d1.wasGeneratedBy(e_notebook, a_generateNotebook)\n",
    "\n",
    "#set_experimentinfo\n",
    "filename = 'ImageClassification_pytorch.ipynb'\n",
    "filesize = get_size(os.path.getsize(filename))\n",
    "creation_time = time.ctime(os.path.getctime(filename))\n",
    "modification_time = time.ctime(os.path.getmtime(filename))\n",
    "\n",
    "e_experimentinfo = d1.entity('ex:Cell Experiment Info', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setexperimentinfo = d1.activity('ex:set_experiment_info()')\n",
    "a_filesize = d1.activity('ex:os.path.getsize()')\n",
    "a_converttime = d1.activity('ex:time.ctime()')\n",
    "a_creationtime = d1.activity('ex:os.path.getctime')\n",
    "a_modificationtime = d1.activity('ex:os.path.getmtime')\n",
    "a_getsize = d1.activity('ex:get_size()')\n",
    "\n",
    "d1.wasStartedBy(a_setexperimentinfo,e_experimentinfo, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_setexperimentinfo, a_creationtime)\n",
    "d1.wasInformedBy(a_setexperimentinfo, a_modificationtime)\n",
    "d1.wasInformedBy(a_filesize, a_getsize)\n",
    "d1.wasInformedBy(a_setexperimentinfo, a_filesize)\n",
    "d1.wasInformedBy(a_creationtime, a_converttime)\n",
    "d1.wasInformedBy(a_modificationtime, a_converttime)\n",
    "\n",
    " \n",
    "d1.hadMember(e_notebook, e_experimentinfo)\n",
    "\n",
    "\n",
    "e_experimentinfo_data = d1.entity(\n",
    "    'ex:Experiment Info Data',(\n",
    "        ('ex:title', filename),\n",
    "\t\t('ex:file size', str(filesize)),\n",
    "\t\t('ex:creation_date', str(creation_time)),\n",
    "        ('ex:modification_date', str(modification_time)),\n",
    "        ('ex:task_type', 'ImageClassification_pytorch'),\n",
    "))\n",
    "d1.wasGeneratedBy(e_experimentinfo_data, a_setexperimentinfo)\n",
    "\n",
    "#set_hardware_info()\n",
    "uname = platform.uname()\n",
    "sysInfo = str(uname.system +' '+ uname.release +' Version: '+ uname.version +' Machine: '+ uname.machine)\n",
    "    \n",
    "svmem = psutil.virtual_memory()\n",
    "\n",
    "GPUs = GPUtil.getGPUs()\n",
    "gpuList = []\n",
    "for gpu in GPUs:\n",
    "    gpu_id = gpu.id\n",
    "    gpu_name = gpu.name\n",
    "    gpuList.append((gpu_id , gpu_name))\n",
    "\n",
    "        \n",
    "e_hardwareinfo = d1.entity('ex:Cell Hardware Info', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_sethardwareinfo = d1.activity('ex:set_hardware_info()')\n",
    "a_platform_uname = d1.activity('ex:platform.uname()')\n",
    "a_cpuinfo = d1.activity('ex:cpuinfo.get_cpu_info()')\n",
    "a_svmemtotal = d1.activity('ex:svmem.total')\n",
    "a_GPUtilgetGPU = d1.activity('ex:GPUtil.getGPUs()')\n",
    "d1.wasStartedBy(a_sethardwareinfo, e_hardwareinfo, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_platform_uname)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_cpuinfo)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_svmemtotal)\n",
    "d1.wasInformedBy(a_svmemtotal, a_getsize)\n",
    "d1.wasInformedBy(a_sethardwareinfo, a_GPUtilgetGPU)\n",
    "d1.hadMember(e_notebook, e_hardwareinfo)\n",
    "e_hardwareinfo_data = d1.entity(\n",
    "    'ex:Hardware Info Data',(\n",
    "        ('ex:CPU', cpuinfo.get_cpu_info()['brand_raw']),\n",
    "        ('ex:RAM',  get_size(svmem.total)),\n",
    "        ('ex:Operating System', sysInfo),\n",
    "        ('ex:GPUs', str(gpuList)),\n",
    "))\n",
    "d1.wasGeneratedBy(e_hardwareinfo_data, a_sethardwareinfo)\n",
    "\n",
    "#set_packages\n",
    "cpuInfo_version = !pip list | grep -i py-cpuinfo\n",
    "pytorch_model_summary_version = !pip list | grep -i pytorch-model-summary\n",
    "\n",
    "\n",
    "e_packages = d1.entity('ex:Cell Packages', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setpackages = d1.activity('ex:set_packages()', )\n",
    "a_getVersion = d1.activity('ex:{package_name}.__version__')\n",
    "a_getVersion_py_cpuinfo = d1.activity('ex:!pip list | grep -i py-cpuinfo')\n",
    "a_getVersion_pytorch_model_summary = d1.activity('ex:!pip list | grep -i pytorch-model-summary')\n",
    "d1.wasStartedBy(a_setpackages, e_packages, time=datetime.datetime.now())\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion)\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion_py_cpuinfo)\n",
    "d1.wasInformedBy(a_setpackages,a_getVersion_pytorch_model_summary)\n",
    "d1.hadMember(e_notebook, e_packages)\n",
    "\n",
    "e_packages_data = d1.entity(\n",
    "    'ex:Packages Data',(\n",
    "        ('ex:numpy', np.__version__),\n",
    "    ('ex:ipywidgets', widgets.__version__),\n",
    "    ('ex:torch', torch.__version__),\n",
    "    ('ex:torchvision', torchvision.__version__),\n",
    "    ('ex:pytorch-ignite', pytorch_ignite.__version__),\n",
    "    ('ex:pytorch-lightning',pytorch_lightning.__version__),\n",
    "    ('ex:gputil', GPUtil.__version__),\n",
    "    ('ex:psutil', psutil.__version__),\n",
    "    ('ex:py-cpuinfo', cpuInfo_version[0]),\n",
    "    ('ex:prov', prov.__version__), \n",
    "))\n",
    "d1.wasGeneratedBy(e_packages_data, a_setpackages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9264a4",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "dataset = getattr(datasets, 'FakeData')\n",
    "training_dataset = dataset(size = 100, image_size = (3, 224, 224), num_classes = 10)\n",
    "testing_dataset = dataset(size = 10, image_size = (3, 224, 224), num_classes = 10)\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "dataInfo = training_dataset.__len__\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Number of datapoints:' in lines:\n",
    "        instances_training = lines\n",
    "        break\n",
    "    else:\n",
    "        instances_training = 'no_data'\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Root location:' in lines:\n",
    "        root_training = lines\n",
    "        break\n",
    "    else:\n",
    "        root_training = 'no_data'\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Split:' in lines:\n",
    "        split_training = lines\n",
    "        break\n",
    "    else:\n",
    "        split_training = 'no_data'\n",
    "e_dataingestion = d1.entity('ex:Cell Data Ingestion', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "\n",
    "\n",
    "a_setdataingestion = d1.activity('ex:set_data_ingestion()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setdataingestion, e_dataingestion)\n",
    "d1.hadMember(e_notebook, e_dataingestion)\n",
    "e_dataingestion_data = d1.entity(\n",
    "    'ex:Data Ingestion Data',(\n",
    "        ('ex:data_format', 'Public dataset'),\n",
    "        ('ex:dataset_id', 'FakeData'),\n",
    "\t\t('ex:description', 'A fake dataset that returns randomly generated images and returns them as PIL images.'),\n",
    "\t\t('ex:classes', 10),\n",
    "\t\t('ex:training_samples', str(instances_training)),\n",
    "\t\t('ex:root_location', str(root_training)),\n",
    "))\n",
    "d1.wasGeneratedBy(e_dataingestion_data, a_setdataingestion)\n",
    "a_splitlines = ('ex:str(dataInfo).splitlines()[2]')\n",
    "a_getlength = d1.activity('ex:{dataset}.__len__()')\n",
    "d1.wasInformedBy(a_setdataingestion, a_getlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019f54c",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(), \n",
    "#transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # grayscale to RGB\n",
    "])\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_datapreparation = d1.entity('ex:Cell Data Preparation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setdatapreparation = d1.activity('ex:set_data_preparation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "\n",
    "d1.wasStartedBy(a_setdatapreparation, e_datapreparation)\n",
    "d1.hadMember(e_notebook, e_datapreparation)\n",
    "e_datapreparation_data = d1.entity(\n",
    "    'ex:Data Preparation Data',(\n",
    "        ('ex:preprocessing', str(transform)),\n",
    "\t\t('ex:number_operations', len(str(transform).splitlines())-2),\n",
    "))\n",
    "d1.wasGeneratedBy(e_datapreparation_data, a_setdatapreparation)\n",
    "d1.wasInfluencedBy(e_datapreparation, e_dataingestion_data)\n",
    "d1.wasInformedBy(a_splitlines, a_getlength)\n",
    "d1.wasInformedBy(a_setdatapreparation, a_splitlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959754a",
   "metadata": {},
   "source": [
    "### Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "# Set up device.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 128\n",
    "print_every = 1  # batches\n",
    "# Wrap in data loader.\n",
    "dataset = getattr(datasets, 'FakeData')\n",
    "training_dataset = dataset(size = 100, image_size = (3, 224, 224), num_classes = 10, transform = transform)\n",
    "testing_dataset = dataset(size = 10, image_size = (3, 224, 224), num_classes = 10, transform = transform)\n",
    "\n",
    "if use_cuda:\n",
    "    kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "else:\n",
    "    kwargs = {}\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "val_loader = None\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "dataInfo = testing_dataset.__len__\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Number of datapoints:' in lines:\n",
    "        instances_testing = lines\n",
    "        break\n",
    "    else:\n",
    "        instances_testing = 'no_data'\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Root location:' in lines:\n",
    "        root_testing = lines\n",
    "        break\n",
    "    else:\n",
    "        root_testing = 'no_data'\n",
    "for lines in str(dataInfo).splitlines():\n",
    "    if 'Split:' in lines:\n",
    "        split_testing = lines\n",
    "        break\n",
    "    else:\n",
    "        split_testing = 'no_data'\n",
    "e_datasegregation = d1.entity('ex:Cell Data Segregation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setdatasegregation = d1.activity('ex:set_data_segregation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setdatasegregation, e_datasegregation)\n",
    "d1.hadMember(e_notebook, e_datasegregation)\n",
    "e_datasegregation_data = d1.entity(\n",
    "    'ex:Data Segregation Data',(\n",
    "        ('ex:training_dataset', str(instances_training + root_training + split_training)),\n",
    "\t\t('ex:testing_dataset', str(instances_testing + root_testing + split_testing)), \n",
    "))\n",
    "d1.wasGeneratedBy(e_datasegregation_data, a_setdatasegregation)\n",
    "d1.wasInfluencedBy(e_datasegregation, e_datapreparation_data)\n",
    "d1.wasInformedBy(a_setdatasegregation, a_getlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbdc84",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "# Set up model, loss, optimizer.\n",
    "lr = 0.001\n",
    "model = models.resnet18(pretrained=0)\n",
    "num_classes = 10\n",
    "model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_modelparameters = d1.entity('ex:Cell Model Parameters', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setmodelparameters = d1.activity('ex:set_model_parameters()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_setmodelparameters, e_modelparameters)\n",
    "d1.hadMember(e_notebook, e_modelparameters)\n",
    "e_modelparameters_data = d1.entity(\n",
    "    'ex:Model Parameters Data',(\n",
    "        ('ex:model_name', 'resnet18'),\n",
    "\t\t('ex:model_description', 'https://arxiv.org/abs/1512.03385'),\n",
    "\t\t('ex:save_checkpoint', 0),\n",
    "\t\t('ex:pretrained', 0),\n",
    "\t\t('ex:gpu_enable', 1),\n",
    "\t\t#('ex:modelParameters', str(model)),\n",
    "\t\t('ex:loss_function', 'CrossEntropyLoss'),\n",
    "\t\t('ex:optimizer', 'Adam'),\n",
    "\t\t('ex:optimizer_learning_rate', lr),\n",
    "\t\t('ex:output_neurons', 10),  \n",
    "))\n",
    "d1.wasGeneratedBy(e_modelparameters_data, a_setmodelparameters)\n",
    "d1.wasInfluencedBy(e_modelparameters, e_datasegregation_data)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddfa45",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a58fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "seed_everything(2, workers=True)\n",
    "def set_train_log(log: str, value: str):\n",
    "    log+= value\n",
    "    return log\n",
    "\n",
    "num_epochs = 3\n",
    "epoch_log = \"\"\n",
    "# Set up pytorch-ignite trainer and evaluator.\n",
    "trainer = create_supervised_trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    device=device,\n",
    "    deterministic=True\n",
    ")\n",
    "metrics = {\n",
    "    \"report\": ClassificationReport(),\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(loss_func),\n",
    "}\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model, metrics=metrics, device=device\n",
    ")\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=print_every))\n",
    "def log_batch(trainer):\n",
    "    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
    "    print(\n",
    "        f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
    "        f\"batch {batch} / {trainer.state.epoch_length}: \"\n",
    "        f\"loss: {trainer.state.output:.3f}\"\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_epoch(trainer):\n",
    "    print(f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \")\n",
    "    train_log = set_train_log('', (f\"Epoch {trainer.state.epoch} / {num_epochs} average results: \"))\n",
    "\n",
    "    def log_results(name, metrics, epoch):\n",
    "        print(\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        )\n",
    "        \n",
    "        log = set_train_log(train_log, (\n",
    "            f\"{name + ':':6} loss: {metrics['loss']:.3f}, \"\n",
    "            f\"accuracy: {metrics['accuracy']:.3f}\"\n",
    "        ))\n",
    "        return log\n",
    "\n",
    "    # Train data.\n",
    "    evaluator.run(train_loader)\n",
    "    log_results(\"train\", evaluator.state.metrics, trainer.state.epoch)\n",
    "    \n",
    "    # Val data.\n",
    "    if val_loader:\n",
    "        evaluator.run(val_loader)\n",
    "        log_results(\"val\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    # Test data.\n",
    "    if test_loader:\n",
    "        evaluator.run(test_loader)\n",
    "        log_results(\"test\", evaluator.state.metrics, trainer.state.epoch)\n",
    "\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    return train_log\n",
    "\n",
    "# Start training.\n",
    "train_log = trainer.run(train_loader, max_epochs=num_epochs)\n",
    "\n",
    "print(train_log)\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "for lines in str(train_log).splitlines():\n",
    "    if 'seed:' in lines:\n",
    "        resulting_model_seed = lines\n",
    "        break\n",
    "    else:\n",
    "        resulting_model_seed = 'no_data'\n",
    "\n",
    "e_training = d1.entity('ex:Cell Training', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_settraining = d1.activity('ex:set_training()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "d1.wasStartedBy(a_settraining, e_training)\n",
    "d1.hadMember(e_notebook, e_training)\n",
    "e_training_data = d1.entity(\n",
    "    'ex:Training Data',(\n",
    "        ('ex:batch_size', batch_size),    \n",
    "\t\t('ex:epochs', num_epochs),\n",
    "\t\t#('ex:train_metrics', str(train_log)),\n",
    "\t\t('ex:print_progress', 1),\n",
    "\t\t('ex:seed', 2),\n",
    "\t\t('ex:resulting_model_seed', resulting_model_seed),\n",
    "))\n",
    "d1.wasGeneratedBy(e_training_data, a_settraining)\n",
    "d1.wasInfluencedBy(e_training, e_modelparameters_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef9610",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "endtime = datetime.datetime.now()\n",
    "executionTime = endtime-starttime\n",
    "\n",
    "e_evaluation = d1.entity('ex:Cell Evaluation', (\n",
    "    ('ex:type', 'notebook_cell'),\n",
    "    ('ex:type', 'p-plan:step'),\n",
    "))\n",
    "a_setevaluation = d1.activity('ex:set_evaluation()', startTime=starttime, endTime=endtime, other_attributes={'prov:executionTime': str(executionTime)})\n",
    "e_evaluation_data = d1.entity(\n",
    "    'ex:Evaluation Data',(\n",
    "        ('ex:accuracy', evaluator.state.metrics['accuracy']),\n",
    "\t\t('ex:loss', evaluator.state.metrics['loss']),\n",
    "))\n",
    "d1.wasStartedBy(a_setevaluation, e_evaluation)\n",
    "d1.hadMember(e_notebook, e_evaluation)\n",
    "d1.wasGeneratedBy(e_evaluation_data, a_setevaluation)\n",
    "d1.wasInfluencedBy(e_evaluation, e_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805defd",
   "metadata": {},
   "source": [
    "### Generate Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add visualization to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "#get time for filenames\n",
    "timestring = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "ProvenanceNameImage = ('Provenance_ImageClassification_' + timestring + '.png')\n",
    "\n",
    "dot = prov_to_dot(d1, direction='RL')\n",
    "dot.write_png('../GeneratedProvenanceData/'+ProvenanceNameImage)\n",
    "\n",
    "provenanceImage_open = widgets.Button(description = 'Open Image File')\n",
    "display(provenanceImage_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenanceImage_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/extension/GeneratedProvenanceData/'+ProvenanceNameImage)\n",
    "\n",
    "provenanceImage_open.on_click(on_button_clicked)\n",
    "Image('../GeneratedProvenanceData/'+ProvenanceNameImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61418d",
   "metadata": {},
   "source": [
    "### Write Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProvenanceName = ('Provenance_ImageClassification_' + timestring + '.json')\n",
    "\n",
    "with open('../GeneratedProvenanceData/'+ProvenanceName, 'w') as prov_file:\n",
    "    prov_file.write(d1.serialize(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa190a3",
   "metadata": {},
   "source": [
    "### Open Provenance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dac221",
   "metadata": {},
   "outputs": [],
   "source": [
    "provenance_open = widgets.Button(description = 'Open Provenance Data File')\n",
    "display(provenance_open)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    provenance_open.on_click = webbrowser.open('http://localhost:8888/lab/tree/extension/GeneratedProvenanceData/'+ProvenanceName)\n",
    "\n",
    "provenance_open.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
